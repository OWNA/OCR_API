We ran XGboost for a multi-class classifier which predicts the label of every
column at a time. We compared different training datasets.

+------------------+---------------+-----------+-------------+----------+
|    Accuracy      |  Train file   | Test file |    Rounds   | Has null |
+------------------+---------------+-----------+-------------+----------+
|      55.3%       |   synthetic   |  train_C  |     200     |    NO    |
+------------------+---------------+-----------+-------------+----------+
|      68.5%       |    train_B    |  train_C  |     200     |    NO    |
+------------------+---------------+-----------+-------------+----------+
|      91.7%       |   train_all   |  K-Fold 5 |    2000     |    NO    |
+------------------+---------------+-----------+-------------+----------+
|      86.9%       |   train_all   |  K-Fold 5 |    4000     |    YES   |
+------------------+---------------+-----------+-------------+----------+

We also ran a rule-based algorithm with poor classification results:

+------------------+---------------+-----------+
|    Accuracy      |  Test file    | Has null  |
+------------------+---------------+-----------+
|      91.4%       |   train_all   |  YES      |
+------------------+---------------+-----------+

The evaluation of the RNN for labeling was based on the files obtained from the
generator.py[#21d4ee19]

We used a standard RNN with multiple bidirectional layers.

+------------------+---------------+-----------+-------------+----------+
|   Accuracy       |  RNN  layers  | RNN size  | Batch size  | Features |
+------------------+---------------+-----------+-------------+----------+
|   93.0%          |        2      |     16    |     1       |    23    |
+------------------+---------------+-----------+-------------+----------+
|   93.2%          |        2      |     32    |     1       |    23    |
+------------------+---------------+-----------+-------------+----------+
|   93.7%          |        2      |     64    |     1       |    23    |
+------------------+---------------+-----------+-------------+----------+
